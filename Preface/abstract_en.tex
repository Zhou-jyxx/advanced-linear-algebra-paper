\thispagestyle{fancy}
\input{paper_info.tex}
\renewcommand\abstractname{\sffamily\zihao{-3} Abstract}
\phantomsection
\begin{abstract}
	\addcontentsline{toc}{section}{Abstract}
	\zihao{5}\rmfamily
	\vspace{\baselineskip}
	With the popularization of computers, optimization method has become a necessary research tool for engineers to resolve large-scale optimization problems. As a subfield of optimization, convex optimization studies the minimization of convex functions defined in convex sets. Because of the improvement of computing power and the full development of optimization theory, convex optimization problem has been widely used in machine learning regarding its excellent properties. In this paper, the convex optimization model is studied. Firstly, some essential mathematical concepts are listed, and the theorems involved are proved in detail. Then, eight optimization methods including Newton-Raphson method, Quasi-Newton method, Gradient descent method, Stochastic gradient descent method, Conjugate gradient method, Proximal gradient method, Augmented Lagrange method, and Alternating direction method of multipliers, are introduced respectively. In each scenario, the convex optimization model applicable to the method is described first, then the corresponding algorithm flow and specific iterative format are presented. Furthermore, the convergence or advantages and disadvantages of the method are analyzed. Finally, classical models such as the Lasso problem, the least square problem, and the matrix decomposition problem are listed. Concrete application examples are presented to increase the reader's understanding of the use of methods in real machine learning problems.
	\par Keywords: Convex Optimization, Machine Learning, Newton's Method, Gradient Method, Multiplier Method
\end{abstract}